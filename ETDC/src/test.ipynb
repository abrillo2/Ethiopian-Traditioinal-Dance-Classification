{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaration of global variables\n",
    "#initialize skeleton models and variables for individual frame skeleton detection\n",
    "protoFile = \"../MPI/pose_deploy_linevec.prototxt\"\n",
    "weightsFile = \"../MPI/pose_iter_160000.caffemodel\"\n",
    "#load pretrained models for single person joint detection\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "jointCount0 = 15\n",
    "POSE_PAIRS =[ [1,0],[1,2],[1,5],[2,3],[3,4],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13],[0,14],[0,15],[14,16],[15,17]]\n",
    "\n",
    "#directories list of dataset for each class\n",
    "category = [cat for cat in os.listdir('../dataset')]\n",
    "'''mutliple dancers joint keypoint extraction weight and network model trained on coco'''\n",
    "\n",
    "#load weight and network files\n",
    "cocoProtoFile = \"../coco/pose_deploy_linevec.prototxt\"\n",
    "cocoCaffeWeightsFile = \"../coco/pose_iter_440000.caffemodel\"\n",
    "jointCount = 18\n",
    "\n",
    "#initialize skeleton pair nodes and labels based on coco training dataset\n",
    "keypointPairs = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],[1,8], [8,9], [9,10], [1,11], [11,12], [12,13],[1,0], [0,14], [14,16], [0,15], [15,17],[2,17], [5,16] ]\n",
    "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip','L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\n",
    "keypointPairsIndex = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44],[19,20], [21,22], [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], [55,56], [37,38], [45,46]]\n",
    "keypointPairsColor = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],[0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],[0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]\n",
    "#initialize the pre-trained coco network using cv dnn, weight from caffe file, network from protofile\n",
    "cocoDnn = cv2.dnn.readNetFromCaffe(cocoProtoFile, cocoCaffeWeightsFile)\n",
    "\n",
    "# Output labels to to classify dance sequence\n",
    "labels = [\"gojjam\",\"guragingna\",\"shoa\"]\n",
    "#modelFile\n",
    "modelFile = \"../models/sample.h5\"\n",
    "sourceFile = \"../test/test1.mkv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real time emotion prediction from specified source\n",
    "def realPred(source,model,labels):\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    sequence = 0\n",
    "    label = \"\"\n",
    "    predicted = False\n",
    "    curentFrameSequenceKeypointList = []\n",
    "    while True:\n",
    "\n",
    "        val, img = liveFrame.read()\n",
    "        \n",
    "        if not val:\n",
    "            continue;\n",
    "        # resize video frames\n",
    "        frame = cv2.resize(img, (640, 360))\n",
    "        \n",
    "        if(sequence < 20 and not predicted):\n",
    "            sequence+=1\n",
    "            currentKeypoints = skeletonizeDancePose(frame,0.1)\n",
    "            curentFrameSequenceKeypointList.append(np.array(currentKeypoints).astype(np.float32))\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            data = (np.array(curentFrameSequenceKeypointList).astype(np.float32))\n",
    "            dancerII = [] \n",
    "            #print(len(curentFrameSequenceKeypointList))\n",
    "            for k in range(len(data)):\n",
    "\n",
    "                currentStep = np.copy(data[k])\n",
    "                currentStep = currentStep.flatten()\n",
    "                dancerII.append(np.array(currentStep).astype(np.float32))\n",
    "            sequenceData = np.array(dancerII).astype(np.float32)\n",
    "            sequenceData = sequenceData[np.newaxis, :, :]\n",
    "            if(not predicted):\n",
    "                \n",
    "                result=model.predict(sequenceData,None)\n",
    "                labelClass= result.argmax(axis=-1)\n",
    "                label=labels[labelClass[0]]\n",
    "                print(\" Label :\",label)\n",
    "                predicted = True\n",
    "                return\n",
    "\n",
    "        testImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(testImg, 1.32, 5)\n",
    "\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=4)\n",
    "\n",
    "            cv2.putText(img, label, (int(x), int(y)), cv2.FONT_HERSHEY_DUPLEX, 1, (120,0,255), 2)\n",
    "\n",
    "        resized_img = cv2.resize(img, (640, 360))\n",
    "        cv2.imshow('Analayzing Dance Form',resized_img)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('e'):\n",
    "            break\n",
    "\n",
    "    liveFrame.release()\n",
    "    cv2.destroyAllWindows\n",
    "\n",
    "#for single person skeleton extraction\n",
    "def skeletonizeDancePose(img, thresh):\n",
    "\n",
    "    frameWidth = img.shape[1]\n",
    "    frameHeight = img.shape[0]\n",
    "    frameCopy = np.copy(img)\n",
    "    threshold = 0.1\n",
    "    \n",
    "    #preprocess img for input\n",
    "    inpBlob = cv2.dnn.blobFromImage(img, 1.0 / 255, (368, 368),(0, 0, 0), swapRB=False, crop=False)\n",
    "    #feed the preprocessed img through the network\n",
    "    cocoDnn.setInput(inpBlob)\n",
    "    output = cocoDnn.forward()\n",
    "    H = output.shape[2]\n",
    "    W = output.shape[3]\n",
    "    \n",
    "    # gather keypoints here\n",
    "    points = []\n",
    "    \n",
    "    #loop through possible 18 keypoints\n",
    "    for i in range(jointCount):\n",
    "        # confidence map of corresponding body's part.\n",
    "        probMap = output[0, i, :, :]\n",
    "\n",
    "        # Find global maxima of the probMap.\n",
    "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "        # Scale the point to fit on the original image\n",
    "        x = (frameWidth * point[0]) / W\n",
    "        y = (frameHeight * point[1]) / H\n",
    "\n",
    "        if prob > thresh :\n",
    "            cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "            cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "            cv2.circle(img, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "            # Add the point to the list if the probability is greater than the threshold\n",
    "            points.append((int(x), int(y)))\n",
    "        else :\n",
    "            points.append((0,0))\n",
    "        \n",
    "    # Draw Skeleton\n",
    "    for pair in POSE_PAIRS:\n",
    "        partA = pair[0]\n",
    "        partB = pair[1]\n",
    "\n",
    "        if points[partA] and points[partB]:\n",
    "            cv2.line(img, points[partA], points[partB], (0, 255, 255), 3)\n",
    "    \n",
    "    return np.array(points)\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    #cv2.imshow(\"Subtraction result\", frame)\n",
    "    \n",
    "    #return the curent 18 body part point pose\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 50, 36) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50, 36), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 20, 36).\n",
      " Label : shoa\n"
     ]
    }
   ],
   "source": [
    "#Load the action-recognition trained model\n",
    "model=load_model(modelFile)\n",
    "data2 = realPred(sourceFile,model,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38695/2942304966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dancerII = [] \n",
    "#print(len(curentFrameSequenceKeypointList))\n",
    "for k in range(len(data2)):\n",
    "                            \n",
    "    currentStep = np.copy(data2[k])\n",
    "    currentStep = currentStep.flatten()\n",
    "    dancerII.append(np.array(currentStep).astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.array(dancerII).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = sequence[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 36)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
